{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Carregamento das bibliotecas que serão utilizadas\n",
    "### + Criação da SESSÃO entre a linguagem Python e o Spark\n",
    "### + Criação do CONTEXTO entre a linguagem SQL-ANSI e o Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verificando o contexto em que se encontra a conexão: \n",
      "\n",
      "<SparkContext master=local[*] appName=PySparkShell>\n",
      "\n",
      " Versão do SPARK em execução: 2.2.0\n"
     ]
    }
   ],
   "source": [
    "#Biblioteca para criar a SESSÃO entre Apache Spark e a linguagem de programação Python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#Biblioteca para criar o CONTEXTO entre o Apache Spark e a linguagem de manipulação de dados SQL-ANSI\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "#Biblioteca para manipulação de linhas sobre o RDD, onde transforma cada linha em um objeto INDEPENDENTE.\n",
    "from pyspark.sql import Row\n",
    "\n",
    "#Biblioteca \n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "#Biblioteca que contêm funções matemáticas\n",
    "import math\n",
    "\n",
    "print('Verificando o contexto em que se encontra a conexão:', '\\n')\n",
    "print(sc) #sc = spark context\n",
    "print('\\n', 'Versão do SPARK em execução:', sc.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.1. SparkSession ->>> trabalhar com DataFrames no SparkSQL, necessário CONSTRUIR uma SESSÃO (explicitamente)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spSession = SparkSession.builder.master(\"local\").appName(\"ML-Spark\")\\\n",
    "          .config(\"spark.some.config.option\", \"some-value\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.2. SQLContext ->>> trabalhar com LINGUAGEM SQL no SparkSQL, necessário CONSTRUIR o CONTEXTO (explicitamente)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlSession = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Carregando o arquivo CSV e mantendo o objeto em cache\n",
    "\n",
    "Será usado a estrutura de RDD para manipulação dos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tipo do objeto criado <rowRDD1>: <class 'pyspark.rdd.RDD'> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#file =\"/home/daniellj/Projetos/Github/DistributedComputing/ApacheSpark/datasets/cars.csv\"\n",
    "file = \"C:\\\\Users\\\\whitecube.daniel\\\\Projetos_Daniel\\\\DistributedComputing\\\\Datasets\\\\cars.csv\"\n",
    "\n",
    "rowRDD1 = sc.textFile(file)\n",
    "rowRDD1.cache() # colocando os dados em cache para maior performance na recuperação dos dados.\n",
    "\n",
    "print('Tipo do objeto criado <rowRDD1>:', type(rowRDD1), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Conhecimento dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imprimindo as 10 primeiras linhas da RDD criado... \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['MAKE,FUELTYPE,ASPIRE,DOORS,BODY,DRIVE,CYLINDERS,HP,RPM,MPG-CITY,MPG-HWY,PRICE',\n",
       " 'subaru,gas,std,two,hatchback,fwd,four,69,4900,31,36,5118',\n",
       " 'chevrolet,gas,std,two,hatchback,fwd,three,48,5100,47,53,5151',\n",
       " 'mazda,gas,std,two,hatchback,fwd,four,68,5000,30,31,5195',\n",
       " 'toyota,gas,std,two,hatchback,fwd,four,62,4800,35,39,5348',\n",
       " 'mitsubishi,gas,std,two,hatchback,fwd,four,68,5500,37,41,5389',\n",
       " 'honda,gas,std,two,hatchback,fwd,four,60,5500,38,42,5399',\n",
       " 'nissan,gas,std,two,sedan,fwd,four,69,5200,31,37,5499',\n",
       " 'dodge,gas,std,two,hatchback,fwd,four,68,5500,37,41,5572',\n",
       " 'plymouth,gas,std,two,hatchback,fwd,four,68,5500,37,41,5572']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Imprimindo as 10 primeiras linhas da RDD criado...', '\\n')\n",
    "rowRDD1.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3. Preparação dos Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1. Removendo o HEADER da RDD..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de registros ANTES de remover o HEADER: 198 \n",
      "\n",
      "Quantidade de registros DEPOIS de remover o HEADER: 197 \n",
      "\n",
      "Exibindo as 5 primeiras linhas da RDD DEPOIS da remeção do HEADER: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['subaru,gas,std,two,hatchback,fwd,four,69,4900,31,36,5118',\n",
       " 'chevrolet,gas,std,two,hatchback,fwd,three,48,5100,47,53,5151',\n",
       " 'mazda,gas,std,two,hatchback,fwd,four,68,5000,30,31,5195',\n",
       " 'toyota,gas,std,two,hatchback,fwd,four,62,4800,35,39,5348',\n",
       " 'mitsubishi,gas,std,two,hatchback,fwd,four,68,5500,37,41,5389']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "header = rowRDD1.first()\n",
    "rowRDD2 = rowRDD1.filter(lambda x : x != header)\n",
    "\n",
    "print('Quantidade de registros ANTES de remover o HEADER:', rowRDD1.count(),'\\n')\n",
    "print('Quantidade de registros DEPOIS de remover o HEADER:', rowRDD2.count(),'\\n')\n",
    "print('Exibindo as 5 primeiras linhas da RDD DEPOIS da remeção do HEADER:', '\\n')\n",
    "rowRDD2.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2. Quebrando os campos após o caractere \",\"\n",
    "#### + convertendo dados nominais em numéricos\n",
    "#### + selecionando as colunas (numéricas) que serão usadas no decorrer do processo...\n",
    "\n",
    "Lembrando que os algoritmos de machine learning foram desenvolvidos para trabalhar com números..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nominalToNumeric(val):\n",
    "    AdjustData = val.split(\",\")\n",
    "    \n",
    "    # na posição 3 da RDD contêm o valor da quant. de portas do carro. Se quant. for expressa pela string \"two\",\n",
    "    # então valor numérico = 1 senão 2\n",
    "    doors = 1.0 if AdjustData[3] == \"two\" else 2.0\n",
    "    \n",
    "    # na posição 4 da RDD contêm o tipo de carroceria do carro. Se Sedan, então valor numérico = 1 senão 2\n",
    "    body = 1.0 if AdjustData[4] == \"sedan\" else 2.0\n",
    "\n",
    "    # Convertendo a RDD para um vetor de linhas + selecionando as colunas que serão usadas no decorrer do processo\n",
    "    values = Row(DOORS = float(doors), BODY = float(body), HP = float(AdjustData[7]), RPM = float(AdjustData[8]), MPG = float(AdjustData[9]))\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# o método persist() vai depender de como está configurado o Starage Level. Possibilidades:\n",
    "    # MEMORY_ONLY\n",
    "    # MEMORY_AND_DISK\n",
    "    # MEMORY_ONLY_SER\n",
    "    # MEMORY_AND_DISK_SER\n",
    "    # DISK_ONLY\n",
    "# Para \"despersistir\" um objeto, base usar o método unpersist().\n",
    "#Fonte: https://data-flair.training/blogs/apache-spark-rdd-persistence-caching/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(BODY=2.0, DOORS=1.0, HP=69.0, MPG=31.0, RPM=4900.0),\n",
       " Row(BODY=2.0, DOORS=1.0, HP=48.0, MPG=47.0, RPM=5100.0),\n",
       " Row(BODY=2.0, DOORS=1.0, HP=68.0, MPG=30.0, RPM=5000.0),\n",
       " Row(BODY=2.0, DOORS=1.0, HP=62.0, MPG=35.0, RPM=4800.0),\n",
       " Row(BODY=2.0, DOORS=1.0, HP=68.0, MPG=37.0, RPM=5500.0),\n",
       " Row(BODY=2.0, DOORS=1.0, HP=60.0, MPG=38.0, RPM=5500.0),\n",
       " Row(BODY=1.0, DOORS=1.0, HP=69.0, MPG=31.0, RPM=5200.0),\n",
       " Row(BODY=2.0, DOORS=1.0, HP=68.0, MPG=37.0, RPM=5500.0),\n",
       " Row(BODY=2.0, DOORS=1.0, HP=68.0, MPG=37.0, RPM=5500.0),\n",
       " Row(BODY=2.0, DOORS=1.0, HP=68.0, MPG=31.0, RPM=5000.0)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rowRDD3 = rowRDD2.map(nominalToNumeric)\n",
    "rowRDD3.persist()\n",
    "rowRDD3.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 3.3. Criando DataFrame a partir da RDD \"ajustada\" anteriormente..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exibindo primeiros registros do DataFrame criado...\n",
      "+----+-----+----+----+------+\n",
      "|BODY|DOORS|  HP| MPG|   RPM|\n",
      "+----+-----+----+----+------+\n",
      "| 2.0|  1.0|69.0|31.0|4900.0|\n",
      "| 2.0|  1.0|48.0|47.0|5100.0|\n",
      "| 2.0|  1.0|68.0|30.0|5000.0|\n",
      "| 2.0|  1.0|62.0|35.0|4800.0|\n",
      "| 2.0|  1.0|68.0|37.0|5500.0|\n",
      "| 2.0|  1.0|60.0|38.0|5500.0|\n",
      "| 1.0|  1.0|69.0|31.0|5200.0|\n",
      "| 2.0|  1.0|68.0|37.0|5500.0|\n",
      "| 2.0|  1.0|68.0|37.0|5500.0|\n",
      "| 2.0|  1.0|68.0|31.0|5000.0|\n",
      "+----+-----+----+----+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rowDF1 = spSession.createDataFrame(rowRDD3)\n",
    "print('Exibindo primeiros registros do DataFrame criado...')\n",
    "rowDF1.show(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
