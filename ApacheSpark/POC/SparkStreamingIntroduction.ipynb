{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1\n",
    "### Simulando a coleta de dados com Spark Streaming através da \"escuta\" em uma porta TCP-IP especificada\n",
    "#### Para isso, iremos usar o netcat como ferramenta de apoio.\n",
    "porta escolhida: 22121\n",
    "\n",
    "abrir o terminal do linux e digitar o comando abaixo:\n",
    "\n",
    "nc -lk 22121"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importando os módulos necessários para o Streaming de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#fonte: https://github.com/danielsan/Spark-Streaming-Examples/blob/master/spark-streaming-foreachRDD-and-foreach.py\n",
    "# Módulos do Spark\n",
    "\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark import SparkContext # quando usamos o PYSPARK, o SPARK CONTEXT já é criado por default: sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Criando o contexto com o Spark Streaming\n",
    "- Lembrando que o contexto com a aplicação Spark, por default pelo PYSPARK já é criado automaticamente com o nome \"sc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-->> Verificando o contexto em que se encontra a conexão: <SparkContext master=local[*] appName=PySparkShell>\n",
      "-->> Versão do SPARK em execução: 2.2.0\n"
     ]
    }
   ],
   "source": [
    "print('-->> Verificando o contexto em que se encontra a conexão:', sc) #sc = spark context\n",
    "print('-->> Versão do SPARK em execução:', sc.version)\n",
    "\n",
    "# Definindo o contexto do Streaming de dados com Spark, uma vez que o contexto com o Spark já foi criado por default\n",
    "strcontext = StreamingContext(sparkContext = sc, batchDuration = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Criando o RECEIVER do Spark.\n",
    "- No caso, estamos usando o socketTextStream por se tratar de uma conexão à uma porta TCP-IP\n",
    "- A coleta de dados é possível através do Twitter, Apache Flume, Apache Kafka, HDFS do Hadoop, IOT: ou seja, as fontes de dados para o RECEIVER que irá \"alimentar\" o Streaming do Spark. Veja, são inúmeras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type object 'lines': <pyspark.streaming.dstream.DStream object at 0x7fbf72168c50>\n"
     ]
    }
   ],
   "source": [
    "# Criando o RECEIVER para fazer o streaming de dados TCP-IP = socketTextStream\n",
    "hostname = \"localhost\"\n",
    "port = 22121\n",
    "\n",
    "lines = strcontext.socketTextStream(hostname = hostname, port = port)\n",
    "print(\"Type object 'lines':\", lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Tratamento e Tranformação\n",
    "### 4.1. Para cada linha, divide as palavras a cada \" \" (espaço) encontrado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type object 'words': <pyspark.streaming.dstream.TransformedDStream object at 0x7fbf72168d30>\n"
     ]
    }
   ],
   "source": [
    "# como estamos executando função de transforamção sobre o DSTREAM gerado (lines), então devemos \"jogar\" o resultado\n",
    "# da transformação em um novo DSTREAM, pois este é sempre IMUTÁVEL.\n",
    "words = lines.flatMap(lambda lines : lines.split(\" \"))\n",
    "\n",
    "print(\"Type object 'words':\", words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Conta o número de ocorrências das palavras em cada batch entregue pelo streaming de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type object 'wordCounts': <class 'pyspark.streaming.dstream.TransformedDStream'>\n"
     ]
    }
   ],
   "source": [
    "pairs = words.map(lambda words : (words, 1))\n",
    "# Exemplo de saída: (('ciência', 1), ('Big Data', 2), ('abacaxi', 1))\n",
    "\n",
    "wordCounts = pairs.reduceByKey(lambda x, y: x + y) # onde a chave é a própria palavra!\n",
    "print(\"Type object 'wordCounts':\", type(wordCounts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Imprimindo os primeiros elementos de cada RDD gerado no DStream\n",
    "RDD = Resilient Distributed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordCounts.pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Início e encerramento da coleta do stream de dados\n",
    "\n",
    "- strcontext.start() = Iniciando a coleta e processamento do stream de dados.\n",
    "- strcontext.awaitTermination() = a coleta de dados por streaming irá rodar indefinidamente até que encontre um erro de execução ou caso finalize todo o trabalho de streaming de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "strcontext.start()\n",
    "strcontext.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2\n",
    "## 1. Usando as técnicas de Windowing\n",
    "\n",
    "Imagem: https://prateekvjoshi.files.wordpress.com/2015/11/2-windowed-processing.png\n",
    "\n",
    "Window Operations: https://spark.apache.org/docs/latest/streaming-programming-guide.html#window-operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Módulos do Spark\n",
    "\n",
    "from pyspark import SparkContext # quando usamos o PYSPARK, o SPARK CONTEXT já é criado por default: sc\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "print('-->> Verificando o contexto em que se encontra a conexão:', sc) #sc = spark context\n",
    "print('-->> Versão do SPARK em execução:', sc.version)\n",
    "\n",
    "def split_words(line):\n",
    "    '''\n",
    "    Input: a linha inteira capturada pelo streaming.\n",
    "    Output: após a captura da linha, faz a quebra após encontrar um \" \" (espaços em branco).\n",
    "            Retorna um dicionário (chave, valor).\n",
    "    '''\n",
    "    try:\n",
    "        if (bool(line.strip()) == True): #verifica se o valor de entrada é:\n",
    "            output = line.strip(\" \")     #um conteúdo não NULO\n",
    "        return(output)\n",
    "    except:\n",
    "        output = ''\n",
    "        return(output)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Definindo o valor de captura do DSTREAM\n",
    "    batchDuration = 1 # tempo em segundos\n",
    "    \n",
    "    # Definindo valores para os parâmetros da WINDOWED\n",
    "    windowLength = 15 * batchDuration # se intervalo do lote = 1 seg, então o tamanho da janela será de 15 RDD's.\n",
    "    slideInterval = 5 * batchDuration  # slideInterval = no momento que ocorrer o evento da window,a partir de que\n",
    "                                 # conjunto de dados deve-se considerar para as trabalhar as transformações e ações\n",
    "                                 # se intervalo do lote = 1 seg, o tamanho da janela = 15, sliding = 5, então\n",
    "                                 # iremos considerar 10 RDD's de um total de 15.\n",
    "\n",
    "    # Definindo o SparkContext\n",
    "    spark_context = sc\n",
    "\n",
    "    # Definindo o contexto do Streaming de dados com Spark, uma vez que o contexto com o Spark já foi criado por default\n",
    "    streaming_context = StreamingContext(sparkContext = spark_context, batchDuration = batchDuration)\n",
    "    streaming_context.checkpoint(\"checkpoint\")\n",
    "\n",
    "    hostname = \"localhost\"\n",
    "    port = 22121\n",
    "\n",
    "    #-->> Chamando a função para construção do RECEIVER\n",
    "    lines = streaming_context.socketTextStream(hostname = hostname, port = port)\n",
    "\n",
    "    #-->> Caso queira chamar via linha de comando e passando os argumentos, considerar comando abaixo:\n",
    "    #lines = streaming_context.socketTextStream(hostname = sys.argv[1], port = int(sys.argv[2]))\n",
    "\n",
    "    #-->> Para cada linha, divide as palavras a cada \" \" (espaço) encontrado\n",
    "    words = lines.flatMap(lambda line : line.split(\" \"))\n",
    "    #-->> 'words' seria algo como = \"abacaxi abacaxi Data \" => [('abacaxi', 'Data', 'abacaxi', '')]\n",
    "\n",
    "    #-->> Conta o número de ocorrências das palavras em cada batch entregue pelo streaming de dados\n",
    "    pairs = words.map(lambda words : (words, 1))\n",
    "    #-->> 'pairs' seria algo como: [('abacaxi', 1), ('abacaxi', 1), ('Data', 1), ('abacaxi', 1), ('', 1)]\n",
    "    \n",
    "    wordCounts = pairs.reduceByKeyAndWindow((lambda x, y: x + y), (lambda x, y: x - y), windowLength, slideInterval) # onde a chave é a própria palavra!\n",
    "    #-->> 'wordsCount' seria algo como: [('abacaxi', 3), ('Data', 1), ('', 1)]\n",
    "    \n",
    "    wordCounts.pprint()\n",
    "    \n",
    "    streaming_context.start()\n",
    "    streaming_context.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Linhas para o streaming coletar:\n",
    "\n",
    "x x x x x x\n",
    "x x x x x x\n",
    "\n",
    "c c c c c c\n",
    "c c c c c c\n",
    "\n",
    "abacaxi abacaxi abacaxi abacaxi abacaxi cera cera cera cera borracha borracha borracha borracha\n",
    "abacaxi abacaxi abacaxi abacaxi abacaxi cera cera cera cera borracha borracha borracha borracha\n",
    "abacaxi abacaxi abacaxi abacaxi abacaxi cera cera cera cera borracha borracha borracha borracha\n",
    "abacaxi abacaxi abacaxi abacaxi abacaxi cera cera cera cera borracha borracha borracha borracha\n",
    "abacaxi abacaxi abacaxi abacaxi abacaxi cera cera cera cera borracha borracha borracha borracha\n",
    "abacaxi abacaxi abacaxi abacaxi abacaxi cera cera cera cera borracha borracha borracha borracha\n",
    "xis xis xis morango morango morango morango morango morango morango morango morango morango morango\n",
    "xis xis xis morango morango morango morango morango morango morango morango morango morango morango\n",
    "xis xis xis morango morango morango\n",
    "xis xis xis morango morango morango morango morango morango morango morango morango morango morango\n",
    "xis xis xis morango morango morango morango morango morango morango morango morango morango morango\n",
    "xis xis xis morango morango morango morango morango\n",
    "moleton moleton moleton camisa camisa camisa\n",
    "teste teste Data Science Data Science Data Science Data Science Data Science\n",
    "teste teste \n",
    "Data Science\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### 2. Encerrando o RECEIVER..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "strcontext.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
