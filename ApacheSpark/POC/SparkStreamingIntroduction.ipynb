{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulando a coleta de dados com Spark Streaming através da \"escuta\" em uma porta TCP-IP especificada\n",
    "#### Para isso, iremos usar o netcat como ferramenta de apoio.\n",
    "porta escolhida: 22121\n",
    "\n",
    "comando: nc -lk 22121"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importando os módulos necessários para o Streaming de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#fonte: https://github.com/danielsan/Spark-Streaming-Examples/blob/master/spark-streaming-foreachRDD-and-foreach.py\n",
    "# Módulos do Spark\n",
    "\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark import SparkContext # quando usamos o PYSPARK, o SPARK CONTEXT já é criado por default: sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Criando o contexto com o Spark Streaming\n",
    "- Lembrando que o contexto com a aplicação Spark, por default pelo PYSPARK já é criado automaticamente com o nome \"sc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-->> Verificando o contexto em que se encontra a conexão:', sc) #sc = spark context\n",
    "print('-->> Versão do SPARK em execução:', sc.version)\n",
    "\n",
    "# Definindo o contexto do Streaming de dados com Spark, uma vez que o contexto com o Spark já foi criado por default\n",
    "strcontext = StreamingContext(sparkContext = sc, batchDuration = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Criando o RECEIVER do Spark.\n",
    "- No caso, estamos usando o socketTextStream por se tratar de uma conexão à uma porta TCP-IP\n",
    "- A coleta de dados é possível através do Twitter, Apache Flume, Apache Kafka, HDFS do Hadoop, IOT: ou seja, as fontes de dados para o RECEIVER que irá \"alimentar\" o Streaming do Spark. Veja, são inúmeras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando o RECEIVER para fazer o streaming de dados TCP-IP = socketTextStream\n",
    "hostname = \"localhost\"\n",
    "port = 22121\n",
    "\n",
    "lines = strcontext.socketTextStream(hostname = hostname, port = port)\n",
    "print(\"Type object 'lines':\", lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Tratamento e Tranformação\n",
    "### 4.1. Para cada linha, divide as palavras a cada \" \" (espaço) encontrado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# como estamos executando função de transforamção sobre o DSTREAM gerado (lines), então devemos \"jogar\" o resultado\n",
    "# da transformação em um novo DSTREAM, pois este é sempre IMUTÁVEL.\n",
    "words = lines.flatMap(lambda lines : lines.split(\" \"))\n",
    "\n",
    "print(\"Type object 'words':\", words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Conta o número de ocorrências das palavras em cada batch entregue pelo streaming de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = words.map(lambda words : (words, 1))\n",
    "# Exemplo de saída: (('ciência', 1), ('Big Data', 2), ('abacaxi', 1))\n",
    "\n",
    "wordCounts = pairs.reduceByKey(lambda x, y: x + y) # onde a chave é a própria palavra!\n",
    "print(\"Type object 'wordCounts':\", type(wordCounts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Imprimindo os 10 primeiros elementos de cada RDD gerado no DStream\n",
    "RDD = Resilient Distributed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordCounts.pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Funções para as etapas de:\n",
    "- Criação do Receiver para o Streaming de dados.\n",
    "- Tranformação dos dados ainda no DStream\n",
    "- Persistência dos dados do DStream para Tupla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Módulos do Spark\n",
    "from pyspark.streaming import StreamingContext\n",
    "#from pyspark import SparkContext # quando usamos o PYSPARK, o SPARK CONTEXT já é criado por default: sc\n",
    "\n",
    "print('-->> Verificando o contexto em que se encontra a conexão:', sc) #sc = spark context\n",
    "print('-->> Versão do SPARK em execução:', sc.version)\n",
    "\n",
    "# Definindo o contexto do Streaming de dados com Spark, uma vez que o contexto com o Spark já foi criado por default\n",
    "strcontext = StreamingContext(sparkContext = sc, batchDuration = 10)\n",
    "\n",
    "hostname = \"localhost\"\n",
    "port = 22121\n",
    "\n",
    "# Criando uma lista vazia\n",
    "values = list()\n",
    "\n",
    "def ReceiverDataStreaming(hostname, port):\n",
    "    '''\n",
    "    Especificação da função...\n",
    "    '''\n",
    "    # Criando o RECEIVER no Streaming Context\n",
    "    lines = strcontext.socketTextStream(hostname = hostname, port = port)\n",
    "    return(lines)\n",
    "\n",
    "def TransformationDataStreaming():\n",
    "    '''\n",
    "    Especificação da função...\n",
    "    '''\n",
    "    # Chamando a função para construção do RECEIVER\n",
    "    lines = ReceiverDataStreaming(hostname = hostname, port = port)\n",
    "\n",
    "    # Para cada linha, divide as palavras a cada \" \" (espaço) encontrado\n",
    "    words = lines.flatMap(lambda lines : lines.split(\" \"))\n",
    "    # 'words' seria algo como = \"abacaxi abacaxi Data \" => [('abacaxi', 'Data', 'abacaxi', '')]\n",
    "\n",
    "    # Conta o número de ocorrências das palavras em cada batch entregue pelo streaming de dados\n",
    "    pairs = words.map(lambda words : (words, 1))\n",
    "    # 'pairs' seria algo como: [('abacaxi', 1), ('abacaxi', 1), ('Data', 1), ('abacaxi', 1), ('', 1)]\n",
    "    \n",
    "    wordsCount = pairs.reduceByKey(lambda x, y: x + y) # onde a chave é a própria palavra!\n",
    "    # 'wordsCount' seria algo como: [('abacaxi', 3), ('Data', 1), ('', 1)]\n",
    "    return(wordsCount)\n",
    "\n",
    "def SendRecord(tup):\n",
    "    '''\n",
    "    Especificação da função...\n",
    "    '''\n",
    "    word   = tup[0]\n",
    "    amount = tup[1]\n",
    "    content = (word, amount)\n",
    "    values.append(content)\n",
    "    #print(values)\n",
    "    # Aqui poderia ser inserido os valores dentro de um banco de dados MongoDB.\n",
    "\n",
    "def PersistDSTream(DSTREAM):\n",
    "    '''\n",
    "    Objetivo: persistir os dados do DSTREAM em uma tupla.\n",
    "    \n",
    "    A parâmetro de entrada será sempre o microbatching gerado pelo DSTREAM (que nada mais é que uma micro coleção\n",
    "    de dados (RDD)!), onde é gerado pela função TransformationDataStreaming().\n",
    "    '''\n",
    "    DSTREAM.foreachRDD(lambda rdd_values : rdd_values.foreach(SendRecord))\n",
    "    return(DSTREAM.pprint())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1. Executando as funções declaradas..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PersistDSTream(DSTREAM = TransformationDataStreaming())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Início e encerramento da coleta do stream de dados\n",
    "\n",
    "- strcontext.start() = Iniciando a coleta e processamento do stream de dados.\n",
    "- strcontext.awaitTermination() = a coleta de dados por streaming irá rodar indefinidamente até que encontre um erro de execução ou caso finalize todo o trabalho de streaming de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "strcontext.start()\n",
    "strcontext.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Encerrando o RECEIVER..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "strcontext.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
