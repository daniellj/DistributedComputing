{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark SQL\n",
    "\n",
    "Pacotes adicionais do Spark: https://spark-packages.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- AppSession criado: <pyspark.sql.session.SparkSession object at 0x7f4c4a5dfef0> \n",
      "\n",
      "- AppContext criado: <pyspark.sql.context.SQLContext object at 0x7f4c4a5684e0> \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.12:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f4c4a5dfef0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# biblioteca para criar a conexão entre o Python e o Apache Spark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "# biblioteca para criar a sessão com a APLICAÇÃO\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# biblioteca para criar a conexão entre a sessão da APLICAÇÃO e o CONTEXTO do Apache Spark + Python\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "from pyspark.sql import Row\n",
    "\n",
    "AppName = 'SparkSQL'\n",
    "MasterNode = 'local'\n",
    "\n",
    "#conf = SparkConf().setAppName(appName).setMaster(master)\n",
    "#sc = SparkContext(conf=conf)\n",
    "\n",
    "#### SparkSession\n",
    "AppSession = SparkSession.builder.master(MasterNode).appName(AppName)\\\n",
    "          .config(\"spark.some.config.option\", \"some-value\").getOrCreate()\n",
    "    \n",
    "#### SQLContext\n",
    "AppContextSpark = SQLContext(AppSession)\n",
    "\n",
    "#### SparkContext\n",
    "print('- AppSession criado:', AppSession, '\\n')\n",
    "print('- AppContext criado:', AppContextSpark, '\\n')\n",
    "AppSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SparkContext -> conexão Python-Spark (pelo Jupyter é criado automáticamente com a chamado do PySpark)\n",
    "\n",
    "Em aplicações profissionais (deploy de solução em PRODUÇÃO), é necessário CONSTRUIR o CONTEXTO (explicitamente)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SparkSession -> para criar a sessão com a APLICAÇÃO (explicitamente)..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQLContext -> para criar a conexão entre a sessão da APLICAÇÃO e o CONTEXTO do Apache Spark + Python (explicitamente)..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importando arquivo CSV e criando a RDD..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tipo do objeto criado <carRDD>: <class 'pyspark.sql.dataframe.DataFrame'>\n",
      "Quantidade de elementos do objeto RDD criado <carRDD> 198 \n",
      "\n",
      "Exibindo as 5 primeiras linhas do RDD criado <carRDD>... \n",
      "\n",
      " [Row(_c0='MAKE', _c1='FUELTYPE', _c2='ASPIRE', _c3='DOORS', _c4='BODY', _c5='DRIVE', _c6='CYLINDERS', _c7='HP', _c8='RPM', _c9='MPG-CITY', _c10='MPG-HWY', _c11='PRICE'), Row(_c0='subaru', _c1='gas', _c2='std', _c3='two', _c4='hatchback', _c5='fwd', _c6='four', _c7='69', _c8='4900', _c9='31', _c10='36', _c11='5118'), Row(_c0='chevrolet', _c1='gas', _c2='std', _c3='two', _c4='hatchback', _c5='fwd', _c6='three', _c7='48', _c8='5100', _c9='47', _c10='53', _c11='5151'), Row(_c0='mazda', _c1='gas', _c2='std', _c3='two', _c4='hatchback', _c5='fwd', _c6='four', _c7='68', _c8='5000', _c9='30', _c10='31', _c11='5195'), Row(_c0='toyota', _c1='gas', _c2='std', _c3='two', _c4='hatchback', _c5='fwd', _c6='four', _c7='62', _c8='4800', _c9='35', _c10='39', _c11='5348')] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "file=\"/home/daniellj/Projetos/Github/DistributedComputing/Datasets/cars.csv\"\n",
    "#file = \"C:\\\\Users\\\\whitecube.daniel\\\\Projetos_Daniel\\\\DistributedComputing\\\\Datasets\\\\cars.csv\"\n",
    "carRDD = AppContextSpark.read.csv(file)\n",
    "\n",
    "print('Tipo do objeto criado <carRDD>:', type(carRDD))\n",
    "print('Quantidade de elementos do objeto RDD criado <carRDD>', carRDD.count(), '\\n')\n",
    "print('Exibindo as 5 primeiras linhas do RDD criado <carRDD>...', '\\n\\n',carRDD.take(5),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD é um repositório de dados genéricos. É possível colocar qualquer coisa dentro de uma RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de elementos onde não há linhas com a expressão \"PRICE\": 197 \n",
      "\n",
      "Ajustando os dados com quebras a partir das vírgulas...exibindo as primeiras linhas após o ajustes...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['subaru',\n",
       "  'gas',\n",
       "  'std',\n",
       "  'two',\n",
       "  'hatchback',\n",
       "  'fwd',\n",
       "  'four',\n",
       "  '69',\n",
       "  '4900',\n",
       "  '31',\n",
       "  '36',\n",
       "  '5118'],\n",
       " ['chevrolet',\n",
       "  'gas',\n",
       "  'std',\n",
       "  'two',\n",
       "  'hatchback',\n",
       "  'fwd',\n",
       "  'three',\n",
       "  '48',\n",
       "  '5100',\n",
       "  '47',\n",
       "  '53',\n",
       "  '5151']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rowRDD2 = carRDD.filter(lambda x : 'PRICE' not in x)\n",
    "print('Quantidade de elementos onde não há linhas com a expressão \"PRICE\":', rowRDD2.count(), '\\n')\n",
    "\n",
    "print('Ajustando os dados com quebras a partir das vírgulas...exibindo as primeiras linhas após o ajustes...')\n",
    "rowRDD3 = rowRDD2.map(lambda x: x.split(','))\n",
    "rowRDD3.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Row can be used to create a row object by using named arguments,\n",
    "the fields will be sorted by names. It is not allowed to omit\n",
    "a named argument to represent the value is None or missing. This should be\n",
    "explicitly set to None in this case.\n",
    "\n",
    ">>> row = Row(name=\"Alice\", age=11)\n",
    ">>> row\n",
    "Row(age=11, name='Alice')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Função Row implementada à seguir: divide o RDD e transforma cada linha em um objeto INDEPENDENTE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rowRDD4 = rowRDD3.map(lambda x: Row(make = x[0], body = x[4], hp = x[7]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exibindo as 20 primeiras linhas independentes criadas <rowRDD4>...\n",
      "Observe que a ordenação dos parâmetros se dá por ordem alfabética.\n",
      "A ordenação dos registros segue o estado original... \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(body='hatchback', hp='69', make='subaru'),\n",
       " Row(body='hatchback', hp='48', make='chevrolet'),\n",
       " Row(body='hatchback', hp='68', make='mazda'),\n",
       " Row(body='hatchback', hp='62', make='toyota'),\n",
       " Row(body='hatchback', hp='68', make='mitsubishi'),\n",
       " Row(body='hatchback', hp='60', make='honda'),\n",
       " Row(body='sedan', hp='69', make='nissan'),\n",
       " Row(body='hatchback', hp='68', make='dodge'),\n",
       " Row(body='hatchback', hp='68', make='plymouth'),\n",
       " Row(body='hatchback', hp='68', make='mazda'),\n",
       " Row(body='hatchback', hp='68', make='mitsubishi'),\n",
       " Row(body='hatchback', hp='68', make='dodge'),\n",
       " Row(body='hatchback', hp='68', make='plymouth'),\n",
       " Row(body='hatchback', hp='70', make='chevrolet'),\n",
       " Row(body='hatchback', hp='62', make='toyota'),\n",
       " Row(body='hatchback', hp='68', make='dodge'),\n",
       " Row(body='hatchback', hp='58', make='honda'),\n",
       " Row(body='hatchback', hp='62', make='toyota'),\n",
       " Row(body='hatchback', hp='76', make='honda'),\n",
       " Row(body='sedan', hp='70', make='chevrolet')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Exibindo as 20 primeiras linhas independentes criadas <rowRDD4>...')\n",
    "print('Observe que a ordenação dos parâmetros se dá por ordem alfabética.')\n",
    "print('A ordenação dos registros segue o estado original...', '\\n')\n",
    "rowRDD4.take(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " ### Criando um dataframe, a partir de uma RDD, usando a SPARK CONTEXT criada anteriormente...\n",
    " \n",
    " Internamente, RDD = DataFrame.\n",
    " Vantagem dos DataFrames: maior facilidade de manipular dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tipo do objeto criado <rowRDD5>: <class 'pyspark.sql.dataframe.DataFrame'> \n",
      "\n",
      "Imprimindo as 15 primeiras linhas do DataFrame criado... \n",
      "\n",
      "+---------+---+----------+\n",
      "|     body| hp|      make|\n",
      "+---------+---+----------+\n",
      "|hatchback| 69|    subaru|\n",
      "|hatchback| 48| chevrolet|\n",
      "|hatchback| 68|     mazda|\n",
      "|hatchback| 62|    toyota|\n",
      "|hatchback| 68|mitsubishi|\n",
      "|hatchback| 60|     honda|\n",
      "|    sedan| 69|    nissan|\n",
      "|hatchback| 68|     dodge|\n",
      "|hatchback| 68|  plymouth|\n",
      "|hatchback| 68|     mazda|\n",
      "|hatchback| 68|mitsubishi|\n",
      "|hatchback| 68|     dodge|\n",
      "|hatchback| 68|  plymouth|\n",
      "|hatchback| 70| chevrolet|\n",
      "|hatchback| 62|    toyota|\n",
      "+---------+---+----------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rowRDD5 = spSession.createDataFrame(rowRDD4)\n",
    "print('Tipo do objeto criado <rowRDD5>:', type(rowRDD5), '\\n')\n",
    "\n",
    "print('Imprimindo as 15 primeiras linhas do DataFrame criado...', '\\n')\n",
    "rowRDD5.show(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### SparkSQL command - Fazendo manipulações de dados do DataFrame criado em SparkSQL..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consulta com select... \n",
      "\n",
      "+---------+---+----------+\n",
      "|     body| hp|      make|\n",
      "+---------+---+----------+\n",
      "|hatchback| 69|    subaru|\n",
      "|hatchback| 48| chevrolet|\n",
      "|hatchback| 68|     mazda|\n",
      "|hatchback| 62|    toyota|\n",
      "|hatchback| 68|mitsubishi|\n",
      "|hatchback| 60|     honda|\n",
      "|    sedan| 69|    nissan|\n",
      "|hatchback| 68|     dodge|\n",
      "|hatchback| 68|  plymouth|\n",
      "|hatchback| 68|     mazda|\n",
      "+---------+---+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Consulta com select...', '\\n')\n",
    "rowRDD5.select('*').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SparkSQL command - Ordenando os dados do DataFrame por uma das colunas..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---+-----------+\n",
      "|       body| hp|       make|\n",
      "+-----------+---+-----------+\n",
      "|  hatchback|154|alfa-romero|\n",
      "|convertible|111|alfa-romero|\n",
      "|convertible|111|alfa-romero|\n",
      "|      sedan|110|       audi|\n",
      "|      sedan|115|       audi|\n",
      "|      sedan|110|       audi|\n",
      "|      wagon|110|       audi|\n",
      "|      sedan|140|       audi|\n",
      "|      sedan|102|       audi|\n",
      "|      sedan|101|        bmw|\n",
      "|      sedan|101|        bmw|\n",
      "|      sedan|121|        bmw|\n",
      "|      sedan|121|        bmw|\n",
      "|      sedan|182|        bmw|\n",
      "|      sedan|182|        bmw|\n",
      "|      sedan|121|        bmw|\n",
      "|      sedan|182|        bmw|\n",
      "|      sedan| 70|  chevrolet|\n",
      "|  hatchback| 70|  chevrolet|\n",
      "|  hatchback| 48|  chevrolet|\n",
      "+-----------+---+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rowRDD5.orderBy('make').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quando queremos utilizar a LINGUAGEM SQL, convertemos o conteúdo de origem (no cenário atual o DataFrame) para uma TABELA TEMPORÁRIA\n",
    "### Convertendo..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rowRDD5.createOrReplaceTempView('rowTB1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A partir do momento que temos a tabela temporária criada, podemos disparar comandos SQL ANSI contra este objeto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disparando uma consulta contra a tabela temporária <rowTB1>... \n",
      "\n",
      "Exibindo todos os registros que sejam da marca \"honda\"... \n",
      "\n",
      "+---------+---+-----+\n",
      "|     body| hp| make|\n",
      "+---------+---+-----+\n",
      "|hatchback| 60|honda|\n",
      "|hatchback| 58|honda|\n",
      "|hatchback| 76|honda|\n",
      "|hatchback| 76|honda|\n",
      "|hatchback| 76|honda|\n",
      "|    sedan| 76|honda|\n",
      "|    wagon| 76|honda|\n",
      "|hatchback| 86|honda|\n",
      "|    sedan| 86|honda|\n",
      "|hatchback| 86|honda|\n",
      "|    sedan| 86|honda|\n",
      "|    sedan|100|honda|\n",
      "|    sedan|101|honda|\n",
      "+---------+---+-----+\n",
      "\n",
      "\n",
      " Exibindo a média de HP dos carros agrupados por marca, onde marcas avaliadas são \"honda\" e \"bmw\"... \n",
      "\n",
      "+-----+------+\n",
      "| make|    hp|\n",
      "+-----+------+\n",
      "|  bmw|138.88|\n",
      "|honda| 80.23|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Disparando uma consulta contra a tabela temporária <rowTB1>...', '\\n')\n",
    "\n",
    "print('Exibindo todos os registros que sejam da marca \"honda\"...', '\\n')\n",
    "spSession.sql('select * from rowTB1 where make = \"honda\"').show()\n",
    "\n",
    "print('\\n', 'Exibindo a média de HP dos carros agrupados por marca, onde marcas avaliadas são \"honda\" e \"bmw\"...', '\\n')\n",
    "spSession.sql('select make, cast(avg(hp) as decimal(10,2)) as hp from rowTB1 where make IN (\"honda\",\"bmw\") group by make').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importando arquivo CSV e criando o DataFrame (diretamente)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tipo do objeto criado <carsDataFrame>: <class 'pyspark.sql.dataframe.DataFrame'> \n",
      "\n",
      "Imprimindo as 10 primeiras linhas do DataFrame criado... \n",
      "\n",
      "+----------+--------+------+-----+---------+-----+---------+---+----+--------+-------+-----+\n",
      "|      MAKE|FUELTYPE|ASPIRE|DOORS|     BODY|DRIVE|CYLINDERS| HP| RPM|MPG-CITY|MPG-HWY|PRICE|\n",
      "+----------+--------+------+-----+---------+-----+---------+---+----+--------+-------+-----+\n",
      "|    subaru|     gas|   std|  two|hatchback|  fwd|     four| 69|4900|      31|     36| 5118|\n",
      "| chevrolet|     gas|   std|  two|hatchback|  fwd|    three| 48|5100|      47|     53| 5151|\n",
      "|     mazda|     gas|   std|  two|hatchback|  fwd|     four| 68|5000|      30|     31| 5195|\n",
      "|    toyota|     gas|   std|  two|hatchback|  fwd|     four| 62|4800|      35|     39| 5348|\n",
      "|mitsubishi|     gas|   std|  two|hatchback|  fwd|     four| 68|5500|      37|     41| 5389|\n",
      "|     honda|     gas|   std|  two|hatchback|  fwd|     four| 60|5500|      38|     42| 5399|\n",
      "|    nissan|     gas|   std|  two|    sedan|  fwd|     four| 69|5200|      31|     37| 5499|\n",
      "|     dodge|     gas|   std|  two|hatchback|  fwd|     four| 68|5500|      37|     41| 5572|\n",
      "|  plymouth|     gas|   std|  two|hatchback|  fwd|     four| 68|5500|      37|     41| 5572|\n",
      "|     mazda|     gas|   std|  two|hatchback|  fwd|     four| 68|5000|      31|     38| 6095|\n",
      "+----------+--------+------+-----+---------+-----+---------+---+----+--------+-------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#file =\"/home/daniellj/Projetos/Github/DistributedComputing/ApacheSpark/datasets/cars.csv\"\n",
    "file = \"C:\\\\Users\\\\whitecube.daniel\\\\Projetos_Daniel\\\\DistributedComputing\\\\Datasets\\\\cars.csv\"\n",
    "\n",
    "rowRDD5 = spSession.read.csv(file, header=True) # header -> informando que o arquivo tem cabeçalho\n",
    "\n",
    "print('Tipo do objeto criado <carsDataFrame>:', type(rowRDD5), '\\n')\n",
    "print('Imprimindo as 10 primeiras linhas do DataFrame criado...', '\\n')\n",
    "rowRDD5.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Com o DataFrame criado, agora novamente iremos criar a TABELA TEMPORÁRIA para poder usar a manipulação com linguagem SQL ANSI..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rowRDD5.createOrReplaceTempView(\"rowTB2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Com a tabela temporária criada, podemos disparar comandos SQL-ANSI sobre o objeto criado..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+-----+\n",
      "|  make| RPM|price|\n",
      "+------+----+-----+\n",
      "| dodge|5000|12964|\n",
      "| dodge|5500| 5572|\n",
      "| dodge|5500| 6229|\n",
      "| dodge|5500| 6377|\n",
      "| dodge|5500| 6692|\n",
      "| dodge|5500| 7609|\n",
      "| dodge|5500| 7957|\n",
      "| dodge|5000| 8921|\n",
      "|subaru|5200|10198|\n",
      "|subaru|4800|11259|\n",
      "|subaru|4800|11694|\n",
      "|subaru|4900| 5118|\n",
      "|subaru|4400| 7053|\n",
      "|subaru|4800| 7126|\n",
      "|subaru|4800| 7463|\n",
      "|subaru|4400| 7603|\n",
      "|subaru|4400| 7775|\n",
      "|subaru|4800| 8013|\n",
      "|subaru|4800| 9233|\n",
      "|subaru|5200| 9960|\n",
      "+------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rowTB3 = spSession.sql('select make, RPM, price from rowTB2 where make IN (\"subaru\", \"dodge\") order by make ASC, price ASC')\n",
    "rowTB3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Trabalhando com arquivo JSON no Apache Spark\n",
    "\n",
    "#### Carregando arquivo JSON para dentro de uma estrutura DataFrame no SparkSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Tipo do objeto criado <employersDF1>: <class 'pyspark.sql.dataframe.DataFrame'> \n",
      "\n",
      "-> Qtd. elementos do DataFrame <employersDF1>: 5 \n",
      "\n",
      "-> Coleção de registros do DataFrame <employersDF1>...\n",
      "+------+-----+----------------+-------+----+\n",
      "|deptid|idade|            nome|salario|sexo|\n",
      "+------+-----+----------------+-------+----+\n",
      "|  1000|   42|   Josias Rebelo|   7000|   m|\n",
      "|  2000|   50|Mauricio Gonheim|   9500|   m|\n",
      "|  1000|   36| Bruno Velasquez|   6700|   m|\n",
      "|  1000|   41|  Ananda Tavares|   9300|   f|\n",
      "|  2000|   34|     Carlos Maia|   5500|   m|\n",
      "+------+-----+----------------+-------+----+\n",
      "\n",
      "-> Schema:\n",
      "root\n",
      " |-- deptid: string (nullable = true)\n",
      " |-- idade: string (nullable = true)\n",
      " |-- nome: string (nullable = true)\n",
      " |-- salario: string (nullable = true)\n",
      " |-- sexo: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file=\"/home/daniellj/Projetos/Github/DistributedComputing/Datasets/employers.json\"\n",
    "#file = \"C:\\\\Users\\\\whitecube.daniel\\\\Projetos_Daniel\\\\DistributedComputing\\\\Datasets\\\\employers.json\"\n",
    "employersDF1 = AppContextSpark.read.json(file)\n",
    "\n",
    "print('-> Tipo do objeto criado <employersDF1>:', type(employersDF1), '\\n')\n",
    "print('-> Qtd. elementos do DataFrame <employersDF1>:', employersDF1.count(), '\\n')\n",
    "print('-> Coleção de registros do DataFrame <employersDF1>...')\n",
    "employersDF1.show()\n",
    "print('-> Schema:'); employersDF1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Sabendo que o arquivo JSON importado virou um objeto do tipo DataFram (pyspark.sql.dataframe.DataFrame), a manipulação de dados é via SQL-ANSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----+\n",
      "|            nome|sexo|\n",
      "+----------------+----+\n",
      "|   Josias Rebelo|   m|\n",
      "|Mauricio Gonheim|   m|\n",
      "| Bruno Velasquez|   m|\n",
      "|  Ananda Tavares|   f|\n",
      "|     Carlos Maia|   m|\n",
      "+----------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employersDF1.select('nome', 'sexo').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+--------------+-------+----+\n",
      "|deptid|idade|          nome|salario|sexo|\n",
      "+------+-----+--------------+-------+----+\n",
      "|  1000|   41|Ananda Tavares|   9300|   f|\n",
      "+------+-----+--------------+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employersDF1.filter(employersDF1['sexo'] == 'f').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------+\n",
      "|sexo|avg(salario)|\n",
      "+----+------------+\n",
      "|   m|      7175.0|\n",
      "|   f|      9300.0|\n",
      "+----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employersDF1.groupBy('sexo').agg({'salario': 'avg'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temp Table\n",
    "A tabela temporária criado à seguir fica disponível apenas para a sessão atual. No momento que a sessão é encerrada, o objeto é desalocado da memória, ou seja, não é um ambiente permanente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+----------------+-------+----+\n",
      "|deptid|idade|            nome|salario|sexo|\n",
      "+------+-----+----------------+-------+----+\n",
      "|  1000|   42|   Josias Rebelo|   7000|   m|\n",
      "|  2000|   50|Mauricio Gonheim|   9500|   m|\n",
      "|  1000|   36| Bruno Velasquez|   6700|   m|\n",
      "|  1000|   41|  Ananda Tavares|   9300|   f|\n",
      "|  2000|   34|     Carlos Maia|   5500|   m|\n",
      "+------+-----+----------------+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Duas formas de criar a Temp Table:\n",
    "\n",
    "#employersDF1.registerTempTable('employersTT1')\n",
    "#AppContextSpark.sql('select * from employersTT1').show()\n",
    "\n",
    "AppContextSpark.registerDataFrameAsTable(df=employersDF1, tableName='employersTT1')\n",
    "AppContextSpark.sql('select * from employersTT1').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Averiguando o tipo da Temp Table criada, é gerado uma mensagem de erro, pois não é um objeto definido e criado no ambiente PySpark. \n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'employersTT1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-9c0530394823>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Averiguando o tipo da Temp Table criada, é gerado uma mensagem de erro, pois não é um objeto definido e criado no ambiente PySpark.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memployersTT1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'employersTT1' is not defined"
     ]
    }
   ],
   "source": [
    "print('Averiguando o tipo da Temp Table criada, é gerado uma mensagem de erro, pois não é um objeto definido e criado no ambiente PySpark.', '\\n')\n",
    "type(employersTT1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convertendo a Temp Table em um objeto persistente..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Tipo do objeto criado: <class 'pyspark.sql.dataframe.DataFrame'> \n",
      "\n",
      "+------+-----+----------------+-------+----+\n",
      "|deptid|idade|            nome|salario|sexo|\n",
      "+------+-----+----------------+-------+----+\n",
      "|  1000|   42|   Josias Rebelo|   7000|   m|\n",
      "|  2000|   50|Mauricio Gonheim|   9500|   m|\n",
      "|  1000|   36| Bruno Velasquez|   6700|   m|\n",
      "|  1000|   41|  Ananda Tavares|   9300|   f|\n",
      "|  2000|   34|     Carlos Maia|   5500|   m|\n",
      "+------+-----+----------------+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employersTT2 = AppContextSpark.table('employersTT1')\n",
    "print('- Tipo do objeto criado:', type(employersTT2), '\\n')\n",
    "employersTT2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparando se o objeto gerado é exatamente IGUAL ao DataFrame de origem..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(employersDF1.collect()) == sorted(employersTT2.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Temp Table..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "AppContextSpark.dropTempTable('employersTT1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Spark + Bancos de Dados Relacionais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conectando em um banco de dados relacional MySQL, (SEM encapsulamento SSH), e efetuando operações com Spark..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CEP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CEP\n",
       "0   53"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "''' SqlAlchemy Config. '''\n",
    "DB_TYPE = 'mysql'\n",
    "DB_DRIVER = 'pymysql'\n",
    "\n",
    "''' Default Config. '''\n",
    "DB_HOST = 'xxx'\n",
    "DB_PORT = 3306\n",
    "DB_USER = 'xxx'\n",
    "DB_PASS = 'xxx'\n",
    "DB_NAME = 'xxx'\n",
    "#CHARSET = 'latin1'\n",
    "#CURSORCLASS = 'pymysql.cursors.Cursor'\n",
    "\n",
    "''' SqlAlchemy Config. '''\n",
    "connection = '%s+%s://%s:%s@%s:%s/%s' % (DB_TYPE, DB_DRIVER, DB_USER, DB_PASS, DB_HOST, DB_PORT, DB_NAME)\n",
    "\n",
    "ENGINE = create_engine(connection, max_overflow=0)\n",
    "query01 = \"SELECT count(CEP) as CEP FROM Endereco;\";\n",
    "\n",
    "result01 = pd.read_sql_query(sql=query01, con=ENGINE, index_col=None, coerce_float=True, params=None, parse_dates=None, chunksize=None)\n",
    "result01.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Spaark + Bancos de Dados NoSQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conectando em um banco de dados NoSQL MongoDB, e efetuando operações..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
